{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rlbook-chapter3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMiLJh03aVmx6JdD9AxplCe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDelg/rl-book/blob/master/chapter4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70I0fdxL6trC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import poisson"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2fRUq4G69Co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Env():\n",
        "    def __init__(\n",
        "        self,\n",
        "        lambda_request_a,\n",
        "        lambda_in_a,\n",
        "        lambda_request_b,\n",
        "        lambda_in_b\n",
        "        ):\n",
        "        self.lambda_request_a = lambda_request_a\n",
        "        self.lambda_in_a = lambda_in_a\n",
        "        self.lambda_request_b = lambda_request_b\n",
        "        self.lambda_in_b = lambda_in_b\n",
        "        self.obs_space = self.ObsSpace()\n",
        "        self.act_space = self.ActionSpace()\n",
        "        self.reset()\n",
        "  \n",
        "    class ObsSpace:\n",
        "        _space = [2]\n",
        "        _min = 0\n",
        "        _max = 5\n",
        "  \n",
        "    class ActionSpace:\n",
        "        _space = [1]\n",
        "        _min = -5\n",
        "        _max = 5\n",
        "  \n",
        "    def move_dynamics(self):\n",
        "        \"\"\"\n",
        "        Returns the matrix probability of terminating on each posible state\n",
        "        given the current state. Each index value represent a combination of \n",
        "        the posible states.\n",
        "        \"\"\"\n",
        "        obs_space_range = self.obs_space._max - self.obs_space._min + 1\n",
        "        arrive_a = poisson.pmf(range(obs_space_range), self.lambda_request_a)\n",
        "        left_a = poisson.pmf(range(obs_space_range), self.lambda_in_a)\n",
        "        arrive_b = poisson.pmf(range(obs_space_range), self.lambda_request_b)\n",
        "        left_b = poisson.pmf(range(obs_space_range), self.lambda_in_b)\n",
        "        diff_prob_a = np.zeros(obs_space_range)\n",
        "        for i in range(obs_space_range):\n",
        "            diff = self.state[0] -i\n",
        "            if diff < 0:\n",
        "                diff_prob_a[i] = (arrive_a[abs(diff):] * left_a[:diff]).sum()\n",
        "            elif diff == 0:\n",
        "                diff_prob_a[i] = (arrive_a * left_a).sum()\n",
        "            else:\n",
        "                diff_prob_a[i] = (arrive_a[:-diff] * left_a[diff:]).sum()\n",
        "\n",
        "        diff_prob_b = np.zeros(obs_space_range)\n",
        "        \n",
        "        for i in range(obs_space_range):\n",
        "            diff = self.state[1] - i\n",
        "            if diff < 0:\n",
        "                diff_prob_b[i] = (arrive_b[abs(diff):] * left_b[:diff]).sum()\n",
        "            elif diff == 0:\n",
        "                diff_prob_b[i] = (arrive_b * left_b).sum()\n",
        "            else:\n",
        "                diff_prob_b[i] = (arrive_b[:-diff] * left_b[diff:]).sum()\n",
        "        return diff_prob_a[:, None] * diff_prob_b[None, :]\n",
        "\n",
        "    def reset(self):\n",
        "        n = int(self.obs_space._max / 2)\n",
        "        self.state = np.array([n, n])\n",
        "  \n",
        "    def _check_min_max(self, state):\n",
        "        for i in range(2):\n",
        "            if state[i] < self.obs_space._min:\n",
        "                state[i] = self.obs_space._min\n",
        "            elif state[i] > self.obs_space._max:\n",
        "                state[i] = self.obs_space._max\n",
        "\n",
        "    def set_state(self, state):\n",
        "        assert len(state) == 2, 'state must be 2d'\n",
        "        self.state = np.array(state)\n",
        "\n",
        "    def _move_cars(self, n):\n",
        "        self.state += np.array([-n , n])\n",
        "        self._check_min_max(self.state)\n",
        "\n",
        "    def reward_dynamics(self):\n",
        "        \"\"\"\n",
        "        Returns the matrix reward from current state.\n",
        "        Each index value represent a combination of the posible states.\n",
        "        \"\"\"\n",
        "        a = np.zeros((self.state[0] + 1, self.state[1] + 1))\n",
        "        with np.nditer(\n",
        "            [a],\n",
        "            flags=['multi_index'],\n",
        "            op_flags=[['readwrite']]) as it:\n",
        "            while not it.finished:\n",
        "                it[0] = np.array(it.multi_index).sum()*10\n",
        "                it.iternext()\n",
        "        a = np.flip(np.flip(a, 1), 0)\n",
        "        obs_space_range = self.obs_space._max - self.obs_space._min + 1\n",
        "        reward_dynamics = np.zeros((obs_space_range, obs_space_range))\n",
        "        reward_dynamics[:self.state[0]+1,:self.state[1]+1] = a\n",
        "        return reward_dynamics\n",
        "    \n",
        "    def dynamics(self, action):\n",
        "        reward = -2*np.abs(action)\n",
        "        self._move_cars(action)\n",
        "        return self.move_dynamics(), (self.reward_dynamics() + reward)\n",
        "\n",
        "    def action(self, action):\n",
        "        reward = -2*np.abs(action)\n",
        "        self._move_cars(action)\n",
        "        request_a = np.random.poisson(self.lambda_request_a)\n",
        "        request_b = np.random.poisson(self.lambda_request_b)\n",
        "        in_a = np.random.poisson(self.lambda_in_a)\n",
        "        in_b = np.random.poisson(self.lambda_in_b)\n",
        "        self.state += np.array([in_a, in_b])\n",
        "        self._check_min_max(self.state)\n",
        "        start_state = self.state.copy()\n",
        "        self.state -= np.array([request_a, request_b])\n",
        "        self._check_min_max(self.state)\n",
        "        reward += np.sum(start_state - self.state)*10\n",
        "        return self.state, reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0o1zWfL9IVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PolicyLearner(object):\n",
        "    def __init__(self, env, gamma=0.995):\n",
        "        self.obs_space = env.obs_space\n",
        "        self.act_space = env.act_space\n",
        "        self.gamma = gamma\n",
        "        self.env = env\n",
        "        self.obs_space_range = self.obs_space._max - self.obs_space._min + 1\n",
        "        self.action_space_range = self.act_space._max - self.act_space._min + 1\n",
        "        self.action_probability = 1./self.action_space_range\n",
        "        self.obs_space_shape = [\n",
        "            self.obs_space_range for _ in range(self.obs_space._space[0])\n",
        "        ]\n",
        "        self.eps = 0.00001\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.value = np.zeros(shape=self.obs_space_shape)\n",
        "        self.policy = np.zeros(shape=self.obs_space_shape, dtype=np.int32)\n",
        "\n",
        "    def policy_evaluation(self):\n",
        "        converged = False\n",
        "        while not converged:\n",
        "            old_value = self.value.copy()\n",
        "            with np.nditer(\n",
        "                [self.value, self.policy],\n",
        "                flags=['multi_index'],\n",
        "                op_flags=[['readwrite'],['readonly']]) as it:\n",
        "                while not it.finished:\n",
        "                    self.env.set_state(it.multi_index)\n",
        "                    next_state_prob, reward_dist = self.env.dynamics(it[1])\n",
        "                    new_value = 0.\n",
        "                    with np.nditer(\n",
        "                        [next_state_prob, reward_dist],\n",
        "                        flags=['multi_index'],\n",
        "                        op_flags=[['readonly'], ['readonly']]) as it2:\n",
        "                        while not it2.finished:     \n",
        "                            new_value += (\n",
        "                                self.action_probability*np.float32(it2[0])*(\n",
        "                                    np.float32(it2[1]) + \n",
        "                                    self.gamma*old_value[tuple(it2.multi_index)]\n",
        "                                )\n",
        "                            it2.iternext()\n",
        "                    it[0][...] = new_value\n",
        "                    it.iternext()\n",
        "            delta = np.sum(np.abs(self.value - old_value))\n",
        "            if delta < self.eps:\n",
        "                converged = True\n",
        "\n",
        "    def policy_improvement(self):\n",
        "        stable = True\n",
        "        with np.nditer(\n",
        "            [self.policy],\n",
        "            flags=['multi_index'],\n",
        "            op_flags=[['readwrite']]) as it:\n",
        "            while not it.finished:\n",
        "                old_action = np.int32(it[0])\n",
        "                q = np.zeros(self.action_space_range)\n",
        "                for i in range(self.action_space_range):\n",
        "                    action = i + self.act_space._min\n",
        "                    self.env.set_state(it.multi_index)\n",
        "                    next_state, reward = self.env.action(action)\n",
        "                    q[i] = self.action_probability*(\n",
        "                            reward + self.gamma*self.value[tuple(next_state)]\n",
        "                        )\n",
        "                it[0][...] = np.argmax(q) + self.act_space._min\n",
        "                if np.int32(it[0]) != old_action:\n",
        "                    stable = False\n",
        "                it.iternext()\n",
        "        return stable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y41BjYvrzJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = Env(3,3,4,2)\n",
        "learner = PolicyLearner(env)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}